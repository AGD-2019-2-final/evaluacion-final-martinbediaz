{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['E', '{(b),(g),(f)}', '[jjj#3,bbb#0,ddd#9,ggg#8,hhh#2]']\n",
      "['A', '{(a),(f),(c)}', '[ccc#2,ddd#0,aaa#3,hhh#9]']\n",
      "['B', '{(f),(e),(a),(c)}', '[ddd#2,ggg#5,ccc#6,jjj#1]']\n",
      "['A', '{(a),(b)}', '[hhh#9,iii#5,eee#7,bbb#1]']\n",
      "['C', '{(f),(g),(d),(a)}', '[iii#6,ddd#5,eee#4,jjj#3]']\n",
      "['A', '{(c),(d)}', '[bbb#2,hhh#0,ccc#4,fff#1,aaa#7]']\n",
      "['A', '{(g),(d),(a)}', '[aaa#5,fff#8,ddd#2,iii#0,jjj#7,ccc#1]']\n",
      "['B', '{(b),(a)}', '[fff#3,hhh#1,ddd#2]']\n",
      "['E', '{(d),(e),(a),(f)}', '[eee#4,ccc#5,iii#9,fff#7,ggg#6,bbb#0]']\n",
      "['B', '{(d),(b),(g),(f)}', '[bbb#7,jjj#9,fff#5,iii#4,ggg#2,eee#3]']\n",
      "['C', '{(d),(c),(f),(b)}', '[hhh#6,eee#4,iii#0,fff#2,jjj#1]']\n",
      "['C', '{(d),(e),(a),(c)}', '[bbb#7,iii#6,ggg#9]']\n",
      "['D', '{(g),(e),(f),(b)}', '[bbb#9,aaa#3,ccc#6,fff#4,eee#2]']\n",
      "['E', '{(c),(f)}', '[aaa#8,ddd#5,jjj#1]']\n",
      "['B', '{(d),(b)}', '[ccc#0,jjj#6,fff#7,ddd#3,aaa#2]']\n",
      "['D', '{(f),(e)}', '[ccc#0,eee#6,bbb#9,ddd#3]']\n",
      "['E', '{(e),(b),(f)}', '[bbb#6,iii#3,hhh#5,fff#4,ggg#9,ddd#2]']\n",
      "['D', '{(g),(a)}', '[hhh#4,jjj#5,ccc#9]']\n",
      "['E', '{(e),(c),(f),(a)}', '[ccc#1,iii#6,fff#9]']\n",
      "['E', '{(e),(a)}', '[bbb#9,aaa#3,fff#1]']\n",
      "['E', '{(e),(f)}', '[ddd#9,iii#2,aaa#4]']\n",
      "['E', '{(c),(b),(g)}', '[ccc#5,fff#8,iii#7]']\n",
      "['D', '{(c),(f),(a)}', '[eee#3,jjj#2,ddd#7]']\n",
      "['A', '{(f),(a),(d)}', '[jjj#1,ggg#0,ccc#7,ddd#9,bbb#3]']\n",
      "['E', '{(c),(d)}', '[jjj#6,ccc#0,aaa#1,hhh#9,iii#7,ggg#8]']\n",
      "['E', '{(e),(d),(c)}', '[fff#3,eee#6,iii#4,bbb#7,ddd#0,ccc#1]']\n",
      "['A', '{(a),(e),(f)}', '[fff#0,ddd#5,ccc#4]']\n",
      "['E', '{(c),(a),(g)}', '[ggg#6,hhh#3,ddd#9,ccc#0,jjj#7]']\n",
      "['A', '{(f),(e)}', '[hhh#6,jjj#0,eee#5,iii#7,ccc#3]']\n",
      "['C', '{(f),(c),(a),(g)}', '[eee#1,fff#4,aaa#2,ccc#7,ggg#0,ddd#6]']\n",
      "['A', '{(b),(f)}', '[ccc#6,aaa#9,eee#5,ddd#0,bbb#3]']\n",
      "['D', '{(b),(f)}', '[bbb#7,hhh#1,aaa#6,iii#4,fff#9,ddd#5]']\n",
      "['E', '{(a),(c)}', '[fff#3,ccc#1,ggg#2,eee#5]']\n",
      "['B', '{(b),(f),(c)}', '[iii#7,ggg#3,ddd#0,jjj#8,hhh#5,ccc#1]']\n",
      "['B', '{(f),(a),(e)}', '[hhh#6,ccc#3,jjj#0,bbb#8,ddd#7]']\n",
      "['D', '{(a),(f)}', '[aaa#0,fff#5,ddd#3]']\n",
      "['B', '{(c),(a)}', '[ddd#5,jjj#2,iii#7,ccc#0,bbb#4]']\n",
      "['C', '{(c),(a),(e),(f)}', '[eee#0,fff#2,hhh#6]']\n",
      "['E', '{(e),(d)}', '[fff#9,iii#2,eee#0]']\n",
      "['E', '{(f),(a),(d)}', '[hhh#8,ggg#3,jjj#5]']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "with open(\"data.tsv\") as tsvfile:\n",
    "    tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "    for line in tsvreader:\n",
    "        print(line[0:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext bigdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeout 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pig_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " fs -rm output/*\n",
      "rm: `output/*': No such file or directory\n",
      " fs -rmdir  output\n",
      "rmdir: `output': No such file or directory\n",
      " fs -rm data.tsv\n",
      "Deleted data.tsv\n",
      " fs -put data.tsv .\n",
      " u = LOAD 'data.tsv' AS (f1:chararray, f2:BAG{t: TUPLE()}, f3:MAP[]);\n",
      " A = FOREACH u GENERATE FLATTEN(f2), FLATTEN(f3);\n",
      " B = FOREACH A GENERATE $0, $1;\n",
      " grouped = GROUP B BY ($0, $1);\n",
      " D = FOREACH grouped GENERATE group, COUNT(B);\n",
      " DUMP D\n",
      "2020-02-15 06:40:55,904 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 06:40:56,099 [JobControl] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 06:40:56,136 [JobControl] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\n",
      "2020-02-15 06:40:56,153 [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1\n",
      "2020-02-15 06:40:56,211 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:1\n",
      "2020-02-15 06:40:56,266 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_1581732162449_0101\n",
      "2020-02-15 06:40:56,269 [JobControl] INFO  org.apache.hadoop.mapred.YARNRunner - Job jar is not present. Not adding any jar to the list of resources.\n",
      "2020-02-15 06:40:56,309 [JobControl] INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl - Submitted application application_1581732162449_0101\n",
      "2020-02-15 06:40:56,317 [JobControl] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://dd8648bbf426:8088/proxy/application_1581732162449_0101/\n",
      "2020-02-15 06:41:31,871 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 06:41:31,898 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 06:41:32,010 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 06:41:32,018 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 06:41:32,065 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 06:41:32,069 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 06:41:32,113 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 06:41:32,118 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 06:41:32,161 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 06:41:32,166 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 06:41:32,207 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 06:41:32,211 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 06:41:32,261 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1\n",
      "((a,aaa),5)\n",
      "((a,bbb),7)\n",
      "((a,ccc),13)\n",
      "((a,ddd),13)\n",
      "((a,eee),7)\n",
      "((a,fff),10)\n",
      "((a,ggg),8)\n",
      "((a,hhh),8)\n",
      "((a,iii),7)\n",
      "((a,jjj),10)\n",
      "((b,aaa),4)\n",
      "((b,bbb),7)\n",
      "((b,ccc),5)\n",
      "((b,ddd),7)\n",
      "((b,eee),5)\n",
      "((b,fff),8)\n",
      "((b,ggg),4)\n",
      "((b,hhh),7)\n",
      "((b,iii),7)\n",
      "((b,jjj),5)\n",
      "((c,aaa),5)\n",
      "((c,bbb),4)\n",
      "((c,ccc),12)\n",
      "((c,ddd),9)\n",
      "((c,eee),6)\n",
      "((c,fff),8)\n",
      "((c,ggg),7)\n",
      "((c,hhh),7)\n",
      "((c,iii),8)\n",
      "((c,jjj),8)\n",
      "((d,aaa),4)\n",
      "((d,bbb),6)\n",
      "((d,ccc),7)\n",
      "((d,ddd),5)\n",
      "((d,eee),6)\n",
      "((d,fff),8)\n",
      "((d,ggg),6)\n",
      "((d,hhh),4)\n",
      "((d,iii),9)\n",
      "((d,jjj),8)\n",
      "((e,aaa),3)\n",
      "((e,bbb),8)\n",
      "((e,ccc),9)\n",
      "((e,ddd),7)\n",
      "((e,eee),7)\n",
      "((e,fff),9)\n",
      "((e,ggg),4)\n",
      "((e,hhh),4)\n",
      "((e,iii),8)\n",
      "((e,jjj),3)\n",
      "((f,aaa),8)\n",
      "((f,bbb),10)\n",
      "((f,ccc),13)\n",
      "((f,ddd),17)\n",
      "((f,eee),11)\n",
      "((f,fff),11)\n",
      "((f,ggg),9)\n",
      "((f,hhh),10)\n",
      "((f,iii),10)\n",
      "((f,jjj),12)\n",
      "((g,aaa),3)\n",
      "((g,bbb),3)\n",
      "((g,ccc),6)\n",
      "((g,ddd),5)\n",
      "((g,eee),4)\n",
      "((g,fff),5)\n",
      "((g,ggg),4)\n",
      "((g,hhh),3)\n",
      "((g,iii),4)\n",
      "((g,jjj),6)\n",
      " --USING PigStorage('\\t') \n",
      "'ddd', f3#'eee', f3#'fff',f3#'ggg',f3#'hhh',f3#'iii', f3#'jjj';#'ccc', f3# \n",
      " --B = FOREACH A GENERATE $0, $1, COUNT(TOBAG($2..));\n",
      " --C = ORDER B BY $0, $1, $2;\n",
      " --STORE C INTO 'output' USING PigStorage(',');\n",
      " --fs -get output/ .\n",
      " --A = LOAD \"input\" USING PigStorage(\",\") AS(f1:int,f2:chararray);;\n",
      " --B = FOREACH A GENERATE f1, FLATTEN(STRSPLIT(f2,\"_\"));\n",
      " --C = FOREACH B GENERATE $0,COUNT(TOBAG($1..));\n",
      " --grouped = GROUP u BY $1;\n",
      " --x = FOREACH u GENERATE COUNT(TOKENIZE($1));\n",
      " --y = FOREACH u GENERATE $0, FLATTEN(TOKENIZE(REPLACE($1,'{', ' ')));\n",
      " --grouped = GROUP x BY $0;\n",
      " --keycount = FOREACH grouped GENERATE group, COUNT(x);\n",
      " --STORE keycount INTO 'output' USING PigStorage(',');\n",
      " --fs -get output/ .\n",
      " --DUMP x;\n"
     ]
    }
   ],
   "source": [
    "%%pig\n",
    "fs -rm output/*\n",
    "fs -rmdir  output\n",
    "fs -rm data.tsv\n",
    "fs -put data.tsv .\n",
    "u = LOAD 'data.tsv' AS (f1:chararray, f2:BAG{t: TUPLE()}, f3:MAP[]);\n",
    "A = FOREACH u GENERATE FLATTEN(f2), FLATTEN(f3);\n",
    "B = FOREACH A GENERATE $0, $1;\n",
    "grouped = GROUP B BY ($0, $1);\n",
    "D = FOREACH grouped GENERATE group, COUNT(B);\n",
    "DUMP D\n",
    "--USING PigStorage('\\t') \n",
    "--A = FOREACH u GENERATE f1, COUNT(f2), f3#'aaa', f3#'bbb', f3#'ccc', f3#'ddd', f3#'eee', f3#'fff',f3#'ggg',f3#'hhh',f3#'iii', f3#'jjj';\n",
    "--B = FOREACH A GENERATE $0, $1, COUNT(TOBAG($2..));\n",
    "--C = ORDER B BY $0, $1, $2;\n",
    " \n",
    "--STORE C INTO 'output' USING PigStorage(',');\n",
    "--fs -get output/ .\n",
    "\n",
    "--A = LOAD \"input\" USING PigStorage(\",\") AS(f1:int,f2:chararray);;\n",
    "--B = FOREACH A GENERATE f1, FLATTEN(STRSPLIT(f2,\"_\"));\n",
    "--C = FOREACH B GENERATE $0,COUNT(TOBAG($1..));\n",
    "--grouped = GROUP u BY $1;\n",
    "--x = FOREACH u GENERATE COUNT(TOKENIZE($1));\n",
    "--y = FOREACH u GENERATE $0, FLATTEN(TOKENIZE(REPLACE($1,'{', ' ')));\n",
    "--grouped = GROUP x BY $0;\n",
    "--keycount = FOREACH grouped GENERATE group, COUNT(x);\n",
    "--STORE keycount INTO 'output' USING PigStorage(',');\n",
    "--fs -get output/ .\n",
    "--DUMP x;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pig_quit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
