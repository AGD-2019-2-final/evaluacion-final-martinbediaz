{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['E', '{(b),(g),(f)}', '[jjj#3,bbb#0,ddd#9,ggg#8,hhh#2]']\n",
      "['A', '{(a),(f),(c)}', '[ccc#2,ddd#0,aaa#3,hhh#9]']\n",
      "['B', '{(f),(e),(a),(c)}', '[ddd#2,ggg#5,ccc#6,jjj#1]']\n",
      "['A', '{(a),(b)}', '[hhh#9,iii#5,eee#7,bbb#1]']\n",
      "['C', '{(f),(g),(d),(a)}', '[iii#6,ddd#5,eee#4,jjj#3]']\n",
      "['A', '{(c),(d)}', '[bbb#2,hhh#0,ccc#4,fff#1,aaa#7]']\n",
      "['A', '{(g),(d),(a)}', '[aaa#5,fff#8,ddd#2,iii#0,jjj#7,ccc#1]']\n",
      "['B', '{(b),(a)}', '[fff#3,hhh#1,ddd#2]']\n",
      "['E', '{(d),(e),(a),(f)}', '[eee#4,ccc#5,iii#9,fff#7,ggg#6,bbb#0]']\n",
      "['B', '{(d),(b),(g),(f)}', '[bbb#7,jjj#9,fff#5,iii#4,ggg#2,eee#3]']\n",
      "['C', '{(d),(c),(f),(b)}', '[hhh#6,eee#4,iii#0,fff#2,jjj#1]']\n",
      "['C', '{(d),(e),(a),(c)}', '[bbb#7,iii#6,ggg#9]']\n",
      "['D', '{(g),(e),(f),(b)}', '[bbb#9,aaa#3,ccc#6,fff#4,eee#2]']\n",
      "['E', '{(c),(f)}', '[aaa#8,ddd#5,jjj#1]']\n",
      "['B', '{(d),(b)}', '[ccc#0,jjj#6,fff#7,ddd#3,aaa#2]']\n",
      "['D', '{(f),(e)}', '[ccc#0,eee#6,bbb#9,ddd#3]']\n",
      "['E', '{(e),(b),(f)}', '[bbb#6,iii#3,hhh#5,fff#4,ggg#9,ddd#2]']\n",
      "['D', '{(g),(a)}', '[hhh#4,jjj#5,ccc#9]']\n",
      "['E', '{(e),(c),(f),(a)}', '[ccc#1,iii#6,fff#9]']\n",
      "['E', '{(e),(a)}', '[bbb#9,aaa#3,fff#1]']\n",
      "['E', '{(e),(f)}', '[ddd#9,iii#2,aaa#4]']\n",
      "['E', '{(c),(b),(g)}', '[ccc#5,fff#8,iii#7]']\n",
      "['D', '{(c),(f),(a)}', '[eee#3,jjj#2,ddd#7]']\n",
      "['A', '{(f),(a),(d)}', '[jjj#1,ggg#0,ccc#7,ddd#9,bbb#3]']\n",
      "['E', '{(c),(d)}', '[jjj#6,ccc#0,aaa#1,hhh#9,iii#7,ggg#8]']\n",
      "['E', '{(e),(d),(c)}', '[fff#3,eee#6,iii#4,bbb#7,ddd#0,ccc#1]']\n",
      "['A', '{(a),(e),(f)}', '[fff#0,ddd#5,ccc#4]']\n",
      "['E', '{(c),(a),(g)}', '[ggg#6,hhh#3,ddd#9,ccc#0,jjj#7]']\n",
      "['A', '{(f),(e)}', '[hhh#6,jjj#0,eee#5,iii#7,ccc#3]']\n",
      "['C', '{(f),(c),(a),(g)}', '[eee#1,fff#4,aaa#2,ccc#7,ggg#0,ddd#6]']\n",
      "['A', '{(b),(f)}', '[ccc#6,aaa#9,eee#5,ddd#0,bbb#3]']\n",
      "['D', '{(b),(f)}', '[bbb#7,hhh#1,aaa#6,iii#4,fff#9,ddd#5]']\n",
      "['E', '{(a),(c)}', '[fff#3,ccc#1,ggg#2,eee#5]']\n",
      "['B', '{(b),(f),(c)}', '[iii#7,ggg#3,ddd#0,jjj#8,hhh#5,ccc#1]']\n",
      "['B', '{(f),(a),(e)}', '[hhh#6,ccc#3,jjj#0,bbb#8,ddd#7]']\n",
      "['D', '{(a),(f)}', '[aaa#0,fff#5,ddd#3]']\n",
      "['B', '{(c),(a)}', '[ddd#5,jjj#2,iii#7,ccc#0,bbb#4]']\n",
      "['C', '{(c),(a),(e),(f)}', '[eee#0,fff#2,hhh#6]']\n",
      "['E', '{(e),(d)}', '[fff#9,iii#2,eee#0]']\n",
      "['E', '{(f),(a),(d)}', '[hhh#8,ggg#3,jjj#5]']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "with open(\"data.tsv\") as tsvfile:\n",
    "    tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "    for line in tsvreader:\n",
    "        print(line[0:])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext bigdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeout 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pig_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " fs -rm output/*\n",
      "rm: `output/*': No such file or directory\n",
      " fs -rmdir  output\n",
      "rmdir: `output': No such file or directory\n",
      " fs -rm data.tsv\n",
      "Deleted data.tsv\n",
      " fs -put data.tsv .\n",
      " -- Carga el archivo desde el disco duro \n",
      " u = LOAD 'data.tsv' AS (f1:CHARARRAY, f2:BAG{t:(p:CHARARRAY)}, f3:MAP[]); \n",
      " x = FOREACH u GENERATE FLATTEN($1);\n",
      " grouped = GROUP x BY $0;\n",
      " keycount = FOREACH grouped GENERATE group, COUNT(x);\n",
      " --STORE keycount INTO 'output' USING PigStorage('\\t');\n",
      " --fs -get output/ .\n",
      " DUMP keycount;\n",
      "2020-02-15 06:21:03,709 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 06:21:04,125 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.reduce.markreset.buffer.percent is deprecated. Instead, use mapreduce.reduce.markreset.buffer.percent\n",
      "2020-02-15 06:21:04,130 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress\n",
      "2020-02-15 06:21:04,153 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "2020-02-15 06:21:04,168 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.submit.replication is deprecated. Instead, use mapreduce.client.submit.file.replication\n",
      "2020-02-15 06:21:04,602 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.tracker.http.address is deprecated. Instead, use mapreduce.jobtracker.http.address\n",
      "2020-02-15 06:21:04,619 [JobControl] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 06:21:04,659 [JobControl] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "2020-02-15 06:21:04,847 [JobControl] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\n",
      "2020-02-15 06:21:04,901 [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1\n",
      "2020-02-15 06:21:05,053 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:1\n",
      "2020-02-15 06:21:05,458 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_1581732162449_0096\n",
      "2020-02-15 06:21:05,785 [JobControl] INFO  org.apache.hadoop.mapred.YARNRunner - Job jar is not present. Not adding any jar to the list of resources.\n",
      "2020-02-15 06:21:05,941 [JobControl] INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl - Submitted application application_1581732162449_0096\n",
      "2020-02-15 06:21:06,010 [JobControl] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://dd8648bbf426:8088/proxy/application_1581732162449_0096/\n",
      "2020-02-15 06:21:36,414 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 06:21:36,432 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 06:21:36,702 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 06:21:36,714 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 06:21:36,809 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 06:21:36,821 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 06:21:36,955 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 06:21:36,963 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 06:21:37,025 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 06:21:37,033 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 06:21:37,094 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 06:21:37,102 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 06:21:37,195 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1\n",
      "(a,22)\n",
      "(b,12)\n",
      "(c,17)\n",
      "(d,13)\n",
      "(e,15)\n",
      "(f,25)\n",
      "(g,9)\n"
     ]
    }
   ],
   "source": [
    "%%pig\n",
    "\n",
    "fs -rm output/*\n",
    "fs -rmdir  output\n",
    "fs -rm data.tsv\n",
    "fs -put data.tsv .\n",
    "-- Carga el archivo desde el disco duro \n",
    "u = LOAD 'data.tsv' AS (f1:CHARARRAY, f2:BAG{t:(p:CHARARRAY)}, f3:MAP[]);\n",
    "x = FOREACH u GENERATE FLATTEN($1);\n",
    "grouped = GROUP x BY $0;\n",
    "keycount = FOREACH grouped GENERATE group, COUNT(x);\n",
    "--STORE keycount INTO 'output' USING PigStorage('\\t');\n",
    "--fs -get output/ .\n",
    "DUMP keycount;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted output/_SUCCESS\n",
      "Deleted output/part-r-00000\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -rm output/*\n",
    "!hadoop fs -rmdir  output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\t22\n",
      "b\t12\n",
      "c\t17\n",
      "d\t13\n",
      "e\t15\n",
      "f\t25\n",
      "g\t9\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -cat output/part-r-00000 | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pig_quit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
