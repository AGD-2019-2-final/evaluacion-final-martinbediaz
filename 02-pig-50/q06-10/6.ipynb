{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['E', '{(b),(g),(f)}', '[jjj#3,bbb#0,ddd#9,ggg#8,hhh#2]']\n",
      "['A', '{(a),(f),(c)}', '[ccc#2,ddd#0,aaa#3,hhh#9]']\n",
      "['B', '{(f),(e),(a),(c)}', '[ddd#2,ggg#5,ccc#6,jjj#1]']\n",
      "['A', '{(a),(b)}', '[hhh#9,iii#5,eee#7,bbb#1]']\n",
      "['C', '{(f),(g),(d),(a)}', '[iii#6,ddd#5,eee#4,jjj#3]']\n",
      "['A', '{(c),(d)}', '[bbb#2,hhh#0,ccc#4,fff#1,aaa#7]']\n",
      "['A', '{(g),(d),(a)}', '[aaa#5,fff#8,ddd#2,iii#0,jjj#7,ccc#1]']\n",
      "['B', '{(b),(a)}', '[fff#3,hhh#1,ddd#2]']\n",
      "['E', '{(d),(e),(a),(f)}', '[eee#4,ccc#5,iii#9,fff#7,ggg#6,bbb#0]']\n",
      "['B', '{(d),(b),(g),(f)}', '[bbb#7,jjj#9,fff#5,iii#4,ggg#2,eee#3]']\n",
      "['C', '{(d),(c),(f),(b)}', '[hhh#6,eee#4,iii#0,fff#2,jjj#1]']\n",
      "['C', '{(d),(e),(a),(c)}', '[bbb#7,iii#6,ggg#9]']\n",
      "['D', '{(g),(e),(f),(b)}', '[bbb#9,aaa#3,ccc#6,fff#4,eee#2]']\n",
      "['E', '{(c),(f)}', '[aaa#8,ddd#5,jjj#1]']\n",
      "['B', '{(d),(b)}', '[ccc#0,jjj#6,fff#7,ddd#3,aaa#2]']\n",
      "['D', '{(f),(e)}', '[ccc#0,eee#6,bbb#9,ddd#3]']\n",
      "['E', '{(e),(b),(f)}', '[bbb#6,iii#3,hhh#5,fff#4,ggg#9,ddd#2]']\n",
      "['D', '{(g),(a)}', '[hhh#4,jjj#5,ccc#9]']\n",
      "['E', '{(e),(c),(f),(a)}', '[ccc#1,iii#6,fff#9]']\n",
      "['E', '{(e),(a)}', '[bbb#9,aaa#3,fff#1]']\n",
      "['E', '{(e),(f)}', '[ddd#9,iii#2,aaa#4]']\n",
      "['E', '{(c),(b),(g)}', '[ccc#5,fff#8,iii#7]']\n",
      "['D', '{(c),(f),(a)}', '[eee#3,jjj#2,ddd#7]']\n",
      "['A', '{(f),(a),(d)}', '[jjj#1,ggg#0,ccc#7,ddd#9,bbb#3]']\n",
      "['E', '{(c),(d)}', '[jjj#6,ccc#0,aaa#1,hhh#9,iii#7,ggg#8]']\n",
      "['E', '{(e),(d),(c)}', '[fff#3,eee#6,iii#4,bbb#7,ddd#0,ccc#1]']\n",
      "['A', '{(a),(e),(f)}', '[fff#0,ddd#5,ccc#4]']\n",
      "['E', '{(c),(a),(g)}', '[ggg#6,hhh#3,ddd#9,ccc#0,jjj#7]']\n",
      "['A', '{(f),(e)}', '[hhh#6,jjj#0,eee#5,iii#7,ccc#3]']\n",
      "['C', '{(f),(c),(a),(g)}', '[eee#1,fff#4,aaa#2,ccc#7,ggg#0,ddd#6]']\n",
      "['A', '{(b),(f)}', '[ccc#6,aaa#9,eee#5,ddd#0,bbb#3]']\n",
      "['D', '{(b),(f)}', '[bbb#7,hhh#1,aaa#6,iii#4,fff#9,ddd#5]']\n",
      "['E', '{(a),(c)}', '[fff#3,ccc#1,ggg#2,eee#5]']\n",
      "['B', '{(b),(f),(c)}', '[iii#7,ggg#3,ddd#0,jjj#8,hhh#5,ccc#1]']\n",
      "['B', '{(f),(a),(e)}', '[hhh#6,ccc#3,jjj#0,bbb#8,ddd#7]']\n",
      "['D', '{(a),(f)}', '[aaa#0,fff#5,ddd#3]']\n",
      "['B', '{(c),(a)}', '[ddd#5,jjj#2,iii#7,ccc#0,bbb#4]']\n",
      "['C', '{(c),(a),(e),(f)}', '[eee#0,fff#2,hhh#6]']\n",
      "['E', '{(e),(d)}', '[fff#9,iii#2,eee#0]']\n",
      "['E', '{(f),(a),(d)}', '[hhh#8,ggg#3,jjj#5]']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "with open(\"data.tsv\") as tsvfile:\n",
    "    tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "    for line in tsvreader:\n",
    "        print(line[0:])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext bigdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeout 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pig_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " fs -rm data.tsv\n",
      "Deleted data.tsv\n",
      " fs -put data.tsv .\n",
      " -- Carga el archivo desde el disco duro \n",
      " u = LOAD 'data.tsv' AS (f1:CHARARRAY, f2:BAG{t:(p:CHARARRAY)}, f3:MAP[]); \n",
      " x = FOREACH u GENERATE FLATTEN($2);\n",
      " grouped = GROUP x BY $0;\n",
      " keycount = FOREACH grouped GENERATE group, COUNT(x);\n",
      " STORE keycount INTO 'output' USING PigStorage(',');\n",
      "2020-02-13 06:10:26,152 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.textoutputformat.separator is deprecated. Instead, use mapreduce.output.textoutputformat.separator\n",
      "2020-02-13 06:10:26,159 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 6000: <line 9, column 0> Output Location Validation Failed for: 'hdfs://0.0.0.0:9000/user/root/output More info to follow:\n",
      "Output directory hdfs://0.0.0.0:9000/user/root/output already exists\n",
      "Details at logfile: /datalake/evaluacion-final-martinbediaz/02-pig-50/q06-10/pig_1581573678438.log\n",
      " fs -get output/ .\n",
      " DUMP keycount;\n",
      "2020-02-13 06:10:26,746 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-13 06:10:27,030 [JobControl] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-13 06:10:27,079 [JobControl] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\n",
      "2020-02-13 06:10:27,163 [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1\n",
      "2020-02-13 06:10:27,269 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:1\n",
      "2020-02-13 06:10:27,433 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_1581569697342_0020\n",
      "2020-02-13 06:10:27,446 [JobControl] INFO  org.apache.hadoop.mapred.YARNRunner - Job jar is not present. Not adding any jar to the list of resources.\n",
      "2020-02-13 06:10:27,844 [JobControl] INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl - Submitted application application_1581569697342_0020\n",
      "2020-02-13 06:10:27,860 [JobControl] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://4fabd5ce6bce:8088/proxy/application_1581569697342_0020/\n",
      "2020-02-13 06:11:03,280 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-13 06:11:03,300 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-13 06:11:03,442 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-13 06:11:03,452 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-13 06:11:03,546 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-13 06:11:03,555 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-13 06:11:03,670 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-13 06:11:03,678 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-13 06:11:03,764 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-13 06:11:03,771 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-13 06:11:03,838 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-13 06:11:03,852 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-13 06:11:03,954 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1\n",
      "(aaa,13)\n",
      "(bbb,16)\n",
      "(ccc,23)\n",
      "(ddd,23)\n",
      "(eee,15)\n",
      "(fff,20)\n",
      "(ggg,13)\n",
      "(hhh,16)\n",
      "(iii,18)\n",
      "(jjj,18)\n"
     ]
    }
   ],
   "source": [
    "%%pig\n",
    "fs -rm data.tsv\n",
    "fs -put data.tsv .\n",
    "-- Carga el archivo desde el disco duro \n",
    "u = LOAD 'data.tsv' AS (f1:CHARARRAY, f2:BAG{t:(p:CHARARRAY)}, f3:MAP[]);\n",
    "x = FOREACH u GENERATE FLATTEN($2);\n",
    "grouped = GROUP x BY $0;\n",
    "keycount = FOREACH grouped GENERATE group, COUNT(x);\n",
    "STORE keycount INTO 'output' USING PigStorage(',');\n",
    "fs -get output/ .\n",
    "DUMP keycount;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted output/_SUCCESS\n",
      "Deleted output/part-r-00000\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -rm output/*\n",
    "!hadoop fs -rmdir  output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%pig_quit` not found.\n"
     ]
    }
   ],
   "source": [
    "%pig_quit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
