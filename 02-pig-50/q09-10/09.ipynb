{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(\"data.tsv\") as tsvfile:\n",
    "    tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "    for line in tsvreader:\n",
    "        print(line[0:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext bigdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeout 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pig_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " fs -rm output/*\n",
      "rm: `output/*': No such file or directory\n",
      " fs -rmdir  output\n",
      "rmdir: `output': No such file or directory\n",
      " fs -rm data.csv\n",
      "Deleted data.csv\n",
      " fs -put data.csv .\n",
      " u = LOAD 'data.csv' USING PigStorage(',');\n",
      " --AS (f1:chararray, f2:BAG{t: TUPLE()}, f3:MAP[]);\n",
      " A = FOREACH u GENERATE $1,$2;\n",
      " --B = FOREACH A GENERATE $0, $1;\n",
      " --grouped = GROUP B BY ($0, $1);\n",
      " --D = FOREACH grouped GENERATE group, COUNT(B);\n",
      " DUMP A\n",
      "2020-02-15 07:17:12,109 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 07:17:12,699 [JobControl] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 07:17:12,724 [JobControl] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\n",
      "2020-02-15 07:17:12,741 [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1\n",
      "2020-02-15 07:17:13,185 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:1\n",
      "2020-02-15 07:17:13,240 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_1581732162449_0109\n",
      "2020-02-15 07:17:13,245 [JobControl] INFO  org.apache.hadoop.mapred.YARNRunner - Job jar is not present. Not adding any jar to the list of resources.\n",
      "2020-02-15 07:17:13,300 [JobControl] INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl - Submitted application application_1581732162449_0109\n",
      "2020-02-15 07:17:13,307 [JobControl] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://dd8648bbf426:8088/proxy/application_1581732162449_0109/\n",
      "2020-02-15 07:17:38,618 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 07:17:38,636 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 07:17:38,725 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 07:17:38,731 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 07:17:38,771 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 07:17:38,775 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 07:17:38,819 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 07:17:38,823 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 07:17:38,866 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 07:17:38,870 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 07:17:38,908 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 07:17:38,913 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 07:17:38,968 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1\n",
      "(Vivian,Hamilton)\n",
      "(Karen,Holcomb)\n",
      "(Cody,Garrett)\n",
      "(Roth,Fry)\n",
      "(Zoe,Conway)\n",
      "(Gretchen,Kinney)\n",
      "(Driscoll,Klein)\n",
      "(Karyn,Diaz)\n",
      "(Merritt,Guy)\n",
      "(Kylan,Sexton)\n",
      "(Jordan,Estes)\n",
      "(Hope,Coffey)\n",
      "(Vivian,Crane)\n",
      "(Clio,Noel)\n",
      "(Hope,Silva)\n",
      "(Ayanna,Jarvis)\n",
      "(Chanda,Boyer)\n",
      "(Chadwick,Knight)\n",
      " --USING PigStorage('\\t') \n",
      "'ddd', f3#'eee', f3#'fff',f3#'ggg',f3#'hhh',f3#'iii', f3#'jjj';#'ccc', f3# \n",
      " --B = FOREACH A GENERATE $0, $1, COUNT(TOBAG($2..));\n",
      " --C = ORDER B BY $0, $1, $2;\n",
      " --STORE C INTO 'output' USING PigStorage(',');\n",
      " --fs -get output/ .\n",
      " --A = LOAD \"input\" USING PigStorage(\",\") AS(f1:int,f2:chararray);;\n",
      " --B = FOREACH A GENERATE f1, FLATTEN(STRSPLIT(f2,\"_\"));\n",
      " --C = FOREACH B GENERATE $0,COUNT(TOBAG($1..));\n",
      " --grouped = GROUP u BY $1;\n",
      " --x = FOREACH u GENERATE COUNT(TOKENIZE($1));\n",
      " --y = FOREACH u GENERATE $0, FLATTEN(TOKENIZE(REPLACE($1,'{', ' ')));\n",
      " --grouped = GROUP x BY $0;\n",
      " --keycount = FOREACH grouped GENERATE group, COUNT(x);\n",
      " STORE A INTO 'output' USING PigStorage('@');\n",
      "2020-02-15 07:17:40,234 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.textoutputformat.separator is deprecated. Instead, use mapreduce.output.textoutputformat.separator\n",
      "2020-02-15 07:17:40,325 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 07:17:40,494 [JobControl] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 07:17:40,517 [JobControl] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\n",
      "2020-02-15 07:17:40,530 [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1\n",
      "2020-02-15 07:17:40,574 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:1\n",
      "2020-02-15 07:17:40,613 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_1581732162449_0110\n",
      "2020-02-15 07:17:40,619 [JobControl] INFO  org.apache.hadoop.mapred.YARNRunner - Job jar is not present. Not adding any jar to the list of resources.\n",
      "2020-02-15 07:17:40,662 [JobControl] INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl - Submitted application application_1581732162449_0110\n",
      "2020-02-15 07:17:40,667 [JobControl] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://dd8648bbf426:8088/proxy/application_1581732162449_0110/\n",
      "2020-02-15 07:18:06,819 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 07:18:06,837 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 07:18:07,009 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 07:18:07,020 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 07:18:07,079 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 07:18:07,089 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 07:18:07,147 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 07:18:07,152 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 07:18:07,200 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 07:18:07,205 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 07:18:07,245 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 07:18:07,249 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      " --fs -get output/ .\n",
      " --DUMP x;\n"
     ]
    }
   ],
   "source": [
    "%%pig\n",
    "fs -rm output/*\n",
    "fs -rmdir  output\n",
    "fs -rm data.csv\n",
    "fs -put data.csv .\n",
    "u = LOAD 'data.csv' USING PigStorage(',');\n",
    "--AS (f1:chararray, f2:BAG{t: TUPLE()}, f3:MAP[]);\n",
    "A = FOREACH u GENERATE $1,$2;\n",
    "DUMP A;\n",
    "\n",
    "STORE A INTO 'output' USING PigStorage('@');\n",
    "\n",
    "\n",
    "--B = FOREACH A GENERATE $0, $1;\n",
    "--grouped = GROUP B BY ($0, $1);\n",
    "--D = FOREACH grouped GENERATE group, COUNT(B);\n",
    "--USING PigStorage('\\t') \n",
    "--A = FOREACH u GENERATE f1, COUNT(f2), f3#'aaa', f3#'bbb', f3#'ccc', f3#'ddd', f3#'eee', f3#'fff',f3#'ggg',f3#'hhh',f3#'iii', f3#'jjj';\n",
    "--B = FOREACH A GENERATE $0, $1, COUNT(TOBAG($2..));\n",
    "--C = ORDER B BY $0, $1, $2;\n",
    " \n",
    "--STORE C INTO 'output' USING PigStorage(',');\n",
    "--fs -get output/ .\n",
    "\n",
    "--A = LOAD \"input\" USING PigStorage(\",\") AS(f1:int,f2:chararray);;\n",
    "--B = FOREACH A GENERATE f1, FLATTEN(STRSPLIT(f2,\"_\"));\n",
    "--C = FOREACH B GENERATE $0,COUNT(TOBAG($1..));\n",
    "--grouped = GROUP u BY $1;\n",
    "--x = FOREACH u GENERATE COUNT(TOKENIZE($1));\n",
    "--y = FOREACH u GENERATE $0, FLATTEN(TOKENIZE(REPLACE($1,'{', ' ')));\n",
    "--grouped = GROUP x BY $0;\n",
    "--keycount = FOREACH grouped GENERATE group, COUNT(x);\n",
    "--STORE A INTO 'output' USING PigStorage('@');\n",
    "--fs -get output/ .\n",
    "--DUMP x;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   1 root supergroup          0 2020-02-15 07:18 output/_SUCCESS\n",
      "-rw-r--r--   1 root supergroup        232 2020-02-15 07:18 output/part-m-00000\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls output/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vivian@Hamilton\n",
      "Karen@Holcomb\n",
      "Cody@Garrett\n",
      "Roth@Fry\n",
      "Zoe@Conway\n",
      "Gretchen@Kinney\n",
      "Driscoll@Klein\n",
      "Karyn@Diaz\n",
      "Merritt@Guy\n",
      "Kylan@Sexton\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -cat output/part-m-00000 | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pig_quit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
