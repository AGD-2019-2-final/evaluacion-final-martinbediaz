{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B', '2017-03-10', '10']\n",
      "['E', '2015-08-02', '11']\n",
      "['B', '2015-09-23', '1']\n",
      "['C', '2015-02-01', '8']\n",
      "['D', '2016-07-03', '4']\n",
      "['D', '2017-08-25', '14']\n",
      "['B', '2017-06-05', '15']\n",
      "['D', '2017-09-27', '13']\n",
      "['B', '2018-06-15', '12']\n",
      "['C', '2017-02-18', '5']\n",
      "['B', '2017-03-09', '2']\n",
      "['D', '2017-11-11', '3']\n",
      "['B', '2017-11-19', '9']\n",
      "['E', '2015-11-28', '6']\n",
      "['D', '2017-11-03', '7']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "with open(\"data.tsv\") as tsvfile:\n",
    "    tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "    for line in tsvreader:\n",
    "        print(line[0:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext bigdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeout 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pig_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " fs -rm data.tsv\n",
      "Deleted data.tsv\n",
      " fs -put data.tsv .\n",
      " -- Carga el archivo desde el disco duro\n",
      "RARRAY, quantity: INT);ING PigStorage('\\t') AS (key: CHARARRAY, fecha: CHA \n",
      " y = ORDER u BY $0, $2, $1;\n",
      " z = FOREACH y GENERATE $0, $2, $1;\n",
      " --STORE y INTO 'output' USING PigStorage('\\t');\n",
      " DUMP y;\n",
      "2020-02-15 05:07:43,940 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 05:07:44,086 [JobControl] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 05:07:44,105 [JobControl] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\n",
      "2020-02-15 05:07:44,119 [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1\n",
      "2020-02-15 05:07:44,557 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:1\n",
      "2020-02-15 05:07:44,610 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_1581732162449_0081\n",
      "2020-02-15 05:07:44,614 [JobControl] INFO  org.apache.hadoop.mapred.YARNRunner - Job jar is not present. Not adding any jar to the list of resources.\n",
      "2020-02-15 05:07:44,670 [JobControl] INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl - Submitted application application_1581732162449_0081\n",
      "2020-02-15 05:07:44,674 [JobControl] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://dd8648bbf426:8088/proxy/application_1581732162449_0081/\n",
      "2020-02-15 05:08:09,899 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 05:08:09,932 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 05:08:10,046 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 05:08:10,055 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 05:08:10,102 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 05:08:10,110 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 05:08:10,300 [JobControl] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 05:08:10,315 [JobControl] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\n",
      "2020-02-15 05:08:10,330 [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1\n",
      "2020-02-15 05:08:10,378 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:1\n",
      "2020-02-15 05:08:10,814 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_1581732162449_0082\n",
      "2020-02-15 05:08:10,818 [JobControl] INFO  org.apache.hadoop.mapred.YARNRunner - Job jar is not present. Not adding any jar to the list of resources.\n",
      "2020-02-15 05:08:10,857 [JobControl] INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl - Submitted application application_1581732162449_0082\n",
      "2020-02-15 05:08:10,862 [JobControl] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://dd8648bbf426:8088/proxy/application_1581732162449_0082/\n",
      "2020-02-15 05:08:45,995 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 05:08:46,020 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 05:08:46,160 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 05:08:46,166 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 05:08:46,206 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 05:08:46,211 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 05:08:46,397 [JobControl] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 05:08:46,413 [JobControl] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\n",
      "2020-02-15 05:08:46,431 [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1\n",
      "2020-02-15 05:08:46,468 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:1\n",
      "2020-02-15 05:08:46,494 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_1581732162449_0083\n",
      "2020-02-15 05:08:46,499 [JobControl] INFO  org.apache.hadoop.mapred.YARNRunner - Job jar is not present. Not adding any jar to the list of resources.\n",
      "2020-02-15 05:08:46,535 [JobControl] INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl - Submitted application application_1581732162449_0083\n",
      "2020-02-15 05:08:46,539 [JobControl] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://dd8648bbf426:8088/proxy/application_1581732162449_0083/\n",
      "2020-02-15 05:09:21,901 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 05:09:21,939 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 05:09:22,063 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 05:09:22,068 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 05:09:22,108 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 05:09:22,112 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 05:09:22,174 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 05:09:22,180 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 05:09:22,219 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 05:09:22,222 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 05:09:22,258 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 05:09:22,261 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 05:09:22,302 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 05:09:22,306 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 05:09:22,343 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 05:09:22,347 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 05:09:22,394 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 05:09:22,397 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 05:09:22,435 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 05:09:22,439 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 05:09:22,478 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 05:09:22,481 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 05:09:22,519 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 05:09:22,523 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 05:09:22,565 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1\n",
      "(B,2015-09-23,1)\n",
      "(B,2017-03-09,2)\n",
      "(B,2017-11-19,9)\n",
      "(B,2017-03-10,10)\n",
      "(B,2018-06-15,12)\n",
      "(B,2017-06-05,15)\n",
      "(C,2017-02-18,5)\n",
      "(C,2015-02-01,8)\n",
      "(D,2017-11-11,3)\n",
      "(D,2016-07-03,4)\n",
      "(D,2017-11-03,7)\n",
      "(D,2017-09-27,13)\n",
      "(D,2017-08-25,14)\n",
      "(E,2015-11-28,6)\n",
      "(E,2015-08-02,11)\n"
     ]
    }
   ],
   "source": [
    "%%pig\n",
    "fs -rm data.tsv\n",
    "fs -put data.tsv .\n",
    "-- Carga el archivo desde el disco duro\n",
    "u = LOAD 'data.tsv' USING PigStorage('\\t') AS (key: CHARARRAY, fecha: CHARARRAY, quantity: INT);\n",
    "y = ORDER u BY $0, $2, $1;\n",
    "z = FOREACH y GENERATE $0, $2, $1;\n",
    "STORE y INTO 'output' USING PigStorage('\\t');\n",
    "DUMP y;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: `output/*': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls output/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: `output/part-r-00000': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -cat output/part-r-00000 | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " fs -get output/ .\n"
     ]
    }
   ],
   "source": [
    "%%pig\n",
    "fs -get output/ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B\t2015-09-23\t1\n",
      "B\t2017-03-09\t2\n",
      "B\t2017-11-19\t9\n",
      "B\t2017-03-10\t10\n",
      "B\t2018-06-15\t12\n",
      "B\t2017-06-05\t15\n",
      "C\t2017-02-18\t5\n",
      "C\t2015-02-01\t8\n",
      "D\t2017-11-11\t3\n",
      "D\t2016-07-03\t4\n",
      "D\t2017-11-03\t7\n",
      "D\t2017-09-27\t13\n",
      "D\t2017-08-25\t14\n",
      "E\t2015-11-28\t6\n",
      "E\t2015-08-02\t11\n"
     ]
    }
   ],
   "source": [
    "!cat output/part-r-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted output/_SUCCESS\n",
      "Deleted output/part-r-00000\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -rm output/*\n",
    "!hadoop fs -rmdir  output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pig_quit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
