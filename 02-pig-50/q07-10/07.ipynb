{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['E', '{(b),(g),(f)}', '[jjj#3,bbb#0,ddd#9,ggg#8,hhh#2]']\n",
      "['A', '{(a),(f),(c)}', '[ccc#2,ddd#0,aaa#3,hhh#9]']\n",
      "['B', '{(f),(e),(a),(c)}', '[ddd#2,ggg#5,ccc#6,jjj#1]']\n",
      "['A', '{(a),(b)}', '[hhh#9,iii#5,eee#7,bbb#1]']\n",
      "['C', '{(f),(g),(d),(a)}', '[iii#6,ddd#5,eee#4,jjj#3]']\n",
      "['A', '{(c),(d)}', '[bbb#2,hhh#0,ccc#4,fff#1,aaa#7]']\n",
      "['A', '{(g),(d),(a)}', '[aaa#5,fff#8,ddd#2,iii#0,jjj#7,ccc#1]']\n",
      "['B', '{(b),(a)}', '[fff#3,hhh#1,ddd#2]']\n",
      "['E', '{(d),(e),(a),(f)}', '[eee#4,ccc#5,iii#9,fff#7,ggg#6,bbb#0]']\n",
      "['B', '{(d),(b),(g),(f)}', '[bbb#7,jjj#9,fff#5,iii#4,ggg#2,eee#3]']\n",
      "['C', '{(d),(c),(f),(b)}', '[hhh#6,eee#4,iii#0,fff#2,jjj#1]']\n",
      "['C', '{(d),(e),(a),(c)}', '[bbb#7,iii#6,ggg#9]']\n",
      "['D', '{(g),(e),(f),(b)}', '[bbb#9,aaa#3,ccc#6,fff#4,eee#2]']\n",
      "['E', '{(c),(f)}', '[aaa#8,ddd#5,jjj#1]']\n",
      "['B', '{(d),(b)}', '[ccc#0,jjj#6,fff#7,ddd#3,aaa#2]']\n",
      "['D', '{(f),(e)}', '[ccc#0,eee#6,bbb#9,ddd#3]']\n",
      "['E', '{(e),(b),(f)}', '[bbb#6,iii#3,hhh#5,fff#4,ggg#9,ddd#2]']\n",
      "['D', '{(g),(a)}', '[hhh#4,jjj#5,ccc#9]']\n",
      "['E', '{(e),(c),(f),(a)}', '[ccc#1,iii#6,fff#9]']\n",
      "['E', '{(e),(a)}', '[bbb#9,aaa#3,fff#1]']\n",
      "['E', '{(e),(f)}', '[ddd#9,iii#2,aaa#4]']\n",
      "['E', '{(c),(b),(g)}', '[ccc#5,fff#8,iii#7]']\n",
      "['D', '{(c),(f),(a)}', '[eee#3,jjj#2,ddd#7]']\n",
      "['A', '{(f),(a),(d)}', '[jjj#1,ggg#0,ccc#7,ddd#9,bbb#3]']\n",
      "['E', '{(c),(d)}', '[jjj#6,ccc#0,aaa#1,hhh#9,iii#7,ggg#8]']\n",
      "['E', '{(e),(d),(c)}', '[fff#3,eee#6,iii#4,bbb#7,ddd#0,ccc#1]']\n",
      "['A', '{(a),(e),(f)}', '[fff#0,ddd#5,ccc#4]']\n",
      "['E', '{(c),(a),(g)}', '[ggg#6,hhh#3,ddd#9,ccc#0,jjj#7]']\n",
      "['A', '{(f),(e)}', '[hhh#6,jjj#0,eee#5,iii#7,ccc#3]']\n",
      "['C', '{(f),(c),(a),(g)}', '[eee#1,fff#4,aaa#2,ccc#7,ggg#0,ddd#6]']\n",
      "['A', '{(b),(f)}', '[ccc#6,aaa#9,eee#5,ddd#0,bbb#3]']\n",
      "['D', '{(b),(f)}', '[bbb#7,hhh#1,aaa#6,iii#4,fff#9,ddd#5]']\n",
      "['E', '{(a),(c)}', '[fff#3,ccc#1,ggg#2,eee#5]']\n",
      "['B', '{(b),(f),(c)}', '[iii#7,ggg#3,ddd#0,jjj#8,hhh#5,ccc#1]']\n",
      "['B', '{(f),(a),(e)}', '[hhh#6,ccc#3,jjj#0,bbb#8,ddd#7]']\n",
      "['D', '{(a),(f)}', '[aaa#0,fff#5,ddd#3]']\n",
      "['B', '{(c),(a)}', '[ddd#5,jjj#2,iii#7,ccc#0,bbb#4]']\n",
      "['C', '{(c),(a),(e),(f)}', '[eee#0,fff#2,hhh#6]']\n",
      "['E', '{(e),(d)}', '[fff#9,iii#2,eee#0]']\n",
      "['E', '{(f),(a),(d)}', '[hhh#8,ggg#3,jjj#5]']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "with open(\"data.tsv\") as tsvfile:\n",
    "    tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "    for line in tsvreader:\n",
    "        print(line[0:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext bigdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeout 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pig_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " fs -rm data.tsv\n",
      "Deleted data.tsv\n",
      " fs -put data.tsv .\n",
      " -- Carga el archivo desde el disco duro \n",
      " u = LOAD 'data.tsv' AS (f1:chararray, f2:BAG{t: TUPLE()}, f3:MAP[]);\n",
      " --USING PigStorage('\\t') \n",
      "dd', f3#'eee', f3#'fff',f3#'ggg',f3#'hhh',f3#'iii', f3#'jjj';#'ccc', f3#'d \n",
      " B = FOREACH A GENERATE $0, $1, COUNT(TOBAG($2..));\n",
      " C = ORDER B BY $0, $1, $2;\n",
      " DUMP B; \n",
      "2020-02-15 04:06:20,109 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 04:06:20,241 [JobControl] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 04:06:20,254 [JobControl] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\n",
      "2020-02-15 04:06:20,263 [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1\n",
      "2020-02-15 04:06:20,297 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:1\n",
      "2020-02-15 04:06:20,342 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_1581732162449_0061\n",
      "2020-02-15 04:06:20,345 [JobControl] INFO  org.apache.hadoop.mapred.YARNRunner - Job jar is not present. Not adding any jar to the list of resources.\n",
      "2020-02-15 04:06:20,580 [JobControl] INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl - Submitted application application_1581732162449_0061\n",
      "2020-02-15 04:06:20,585 [JobControl] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://dd8648bbf426:8088/proxy/application_1581732162449_0061/\n",
      "2020-02-15 04:06:46,029 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 04:06:46,047 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 04:06:46,160 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 04:06:46,166 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 04:06:46,200 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 04:06:46,206 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 04:06:46,253 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 04:06:46,257 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 04:06:46,298 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 04:06:46,301 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 04:06:46,339 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 04:06:46,342 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 04:06:46,386 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1\n",
      "(E,3,5)\n",
      "(A,3,4)\n",
      "(B,4,4)\n",
      "(A,2,4)\n",
      "(C,4,4)\n",
      "(A,2,5)\n",
      "(A,3,6)\n",
      "(B,2,3)\n",
      "(E,4,6)\n",
      "(B,4,6)\n",
      "(C,4,5)\n",
      "(C,4,3)\n",
      "(D,4,5)\n",
      "(E,2,3)\n",
      "(B,2,5)\n",
      "(D,2,4)\n",
      "(E,3,6)\n",
      "(D,2,3)\n",
      "(E,4,3)\n",
      "(E,2,3)\n",
      "(E,2,3)\n",
      "(E,3,3)\n",
      "(D,3,3)\n",
      "(A,3,5)\n",
      "(E,2,6)\n",
      "(E,3,6)\n",
      "(A,3,3)\n",
      "(E,3,5)\n",
      "(A,2,5)\n",
      "(C,4,6)\n",
      "(A,2,5)\n",
      "(D,2,6)\n",
      "(E,2,4)\n",
      "(B,3,6)\n",
      "(B,3,5)\n",
      "(D,2,3)\n",
      "(B,2,5)\n",
      "(C,4,3)\n",
      "(E,2,3)\n",
      "(E,3,3)\n",
      " --STORE C INTO 'output' USING PigStorage(',');\n",
      " --fs -get output/ .\n",
      " --A = LOAD \"input\" USING PigStorage(\",\") AS(f1:int,f2:chararray);;\n",
      " --B = FOREACH A GENERATE f1, FLATTEN(STRSPLIT(f2,\"_\"));\n",
      " --C = FOREACH B GENERATE $0,COUNT(TOBAG($1..));\n",
      " --grouped = GROUP u BY $1;\n",
      " --x = FOREACH u GENERATE COUNT(TOKENIZE($1));\n",
      " --y = FOREACH u GENERATE $0, FLATTEN(TOKENIZE(REPLACE($1,'{', ' ')));\n",
      " --grouped = GROUP x BY $0;\n",
      " --keycount = FOREACH grouped GENERATE group, COUNT(x);\n",
      " --STORE keycount INTO 'output' USING PigStorage(',');\n",
      " --fs -get output/ .\n",
      " --DUMP x;\n"
     ]
    }
   ],
   "source": [
    "%%pig\n",
    "fs -rm data.tsv\n",
    "fs -put data.tsv .\n",
    "-- Carga el archivo desde el disco duro \n",
    "u = LOAD 'data.tsv' AS (f1:chararray, f2:BAG{t: TUPLE()}, f3:MAP[]);\n",
    "--USING PigStorage('\\t') \n",
    "A = FOREACH u GENERATE f1, COUNT(f2), f3#'aaa', f3#'bbb', f3#'ccc', f3#'ddd', f3#'eee', f3#'fff',f3#'ggg',f3#'hhh',f3#'iii', f3#'jjj';\n",
    "B = FOREACH A GENERATE $0, $1, COUNT(TOBAG($2..));\n",
    "C = ORDER B BY $0, $1, $2;\n",
    "DUMP B; \n",
    "--STORE C INTO 'output' USING PigStorage(',');\n",
    "--fs -get output/ .\n",
    "\n",
    "--A = LOAD \"input\" USING PigStorage(\",\") AS(f1:int,f2:chararray);;\n",
    "--B = FOREACH A GENERATE f1, FLATTEN(STRSPLIT(f2,\"_\"));\n",
    "--C = FOREACH B GENERATE $0,COUNT(TOBAG($1..));\n",
    "--grouped = GROUP u BY $1;\n",
    "--x = FOREACH u GENERATE COUNT(TOKENIZE($1));\n",
    "--y = FOREACH u GENERATE $0, FLATTEN(TOKENIZE(REPLACE($1,'{', ' ')));\n",
    "--grouped = GROUP x BY $0;\n",
    "--keycount = FOREACH grouped GENERATE group, COUNT(x);\n",
    "--STORE keycount INTO 'output' USING PigStorage(',');\n",
    "--fs -get output/ .\n",
    "--DUMP x;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " DUMP C; \n",
      "2020-02-15 04:07:28,285 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 04:07:28,443 [JobControl] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 04:07:28,460 [JobControl] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\n",
      "2020-02-15 04:07:28,469 [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1\n",
      "2020-02-15 04:07:28,918 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:1\n",
      "2020-02-15 04:07:28,960 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_1581732162449_0062\n",
      "2020-02-15 04:07:28,964 [JobControl] INFO  org.apache.hadoop.mapred.YARNRunner - Job jar is not present. Not adding any jar to the list of resources.\n",
      "2020-02-15 04:07:28,994 [JobControl] INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl - Submitted application application_1581732162449_0062\n",
      "2020-02-15 04:07:28,999 [JobControl] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://dd8648bbf426:8088/proxy/application_1581732162449_0062/\n",
      "2020-02-15 04:07:54,384 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 04:07:54,435 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 04:07:54,648 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 04:07:54,659 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 04:07:54,694 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 04:07:54,698 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 04:07:54,883 [JobControl] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 04:07:54,895 [JobControl] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\n",
      "2020-02-15 04:07:54,908 [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1\n",
      "2020-02-15 04:07:54,947 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:1\n",
      "2020-02-15 04:07:55,385 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_1581732162449_0063\n",
      "2020-02-15 04:07:55,393 [JobControl] INFO  org.apache.hadoop.mapred.YARNRunner - Job jar is not present. Not adding any jar to the list of resources.\n",
      "2020-02-15 04:07:55,426 [JobControl] INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl - Submitted application application_1581732162449_0063\n",
      "2020-02-15 04:07:55,432 [JobControl] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://dd8648bbf426:8088/proxy/application_1581732162449_0063/\n",
      "2020-02-15 04:08:30,712 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 04:08:30,730 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 04:08:30,851 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 04:08:30,855 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 04:08:30,893 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 04:08:30,899 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 04:08:31,178 [JobControl] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 04:08:31,202 [JobControl] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\n",
      "2020-02-15 04:08:31,220 [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1\n",
      "2020-02-15 04:08:31,674 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:1\n",
      "2020-02-15 04:08:31,766 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_1581732162449_0064\n",
      "2020-02-15 04:08:31,772 [JobControl] INFO  org.apache.hadoop.mapred.YARNRunner - Job jar is not present. Not adding any jar to the list of resources.\n",
      "2020-02-15 04:08:31,833 [JobControl] INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl - Submitted application application_1581732162449_0064\n",
      "2020-02-15 04:08:31,840 [JobControl] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://dd8648bbf426:8088/proxy/application_1581732162449_0064/\n",
      "2020-02-15 04:09:07,148 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 04:09:07,168 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 04:09:07,287 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 04:09:07,292 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 04:09:07,366 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 04:09:07,370 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 04:09:07,431 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 04:09:07,438 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 04:09:07,495 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 04:09:07,501 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 04:09:07,535 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 04:09:07,538 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 04:09:07,574 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 04:09:07,579 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 04:09:07,626 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 04:09:07,629 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 04:09:07,663 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 04:09:07,666 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 04:09:07,702 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 04:09:07,706 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 04:09:07,742 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 04:09:07,746 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 04:09:07,784 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-02-15 04:09:07,788 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-02-15 04:09:07,826 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1\n",
      "(A,2,4)\n",
      "(A,2,5)\n",
      "(A,2,5)\n",
      "(A,2,5)\n",
      "(A,3,3)\n",
      "(A,3,4)\n",
      "(A,3,5)\n",
      "(A,3,6)\n",
      "(B,2,3)\n",
      "(B,2,5)\n",
      "(B,2,5)\n",
      "(B,3,5)\n",
      "(B,3,6)\n",
      "(B,4,4)\n",
      "(B,4,6)\n",
      "(C,4,3)\n",
      "(C,4,3)\n",
      "(C,4,4)\n",
      "(C,4,5)\n",
      "(C,4,6)\n",
      "(D,2,3)\n",
      "(D,2,3)\n",
      "(D,2,4)\n",
      "(D,2,6)\n",
      "(D,3,3)\n",
      "(D,4,5)\n",
      "(E,2,3)\n",
      "(E,2,3)\n",
      "(E,2,3)\n",
      "(E,2,3)\n",
      "(E,2,4)\n",
      "(E,2,6)\n",
      "(E,3,3)\n",
      "(E,3,3)\n",
      "(E,3,5)\n",
      "(E,3,5)\n",
      "(E,3,6)\n",
      "(E,3,6)\n",
      "(E,4,3)\n",
      "(E,4,6)\n"
     ]
    }
   ],
   "source": [
    "%%pig\n",
    "DUMP C; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted output/_SUCCESS\n",
      "Deleted output/part-r-00000\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -rm output/*\n",
    "!hadoop fs -rmdir  output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pig_quit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
